{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1） Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://huggingface.co/datasets/YuAnthony/tnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaModel\n",
    "from transformers import AdamW\n",
    "# from transformers import BertPreTrainedModel, BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    train_data_path = './tnews/train.json'\n",
    "    dev_data_path = './tnews/dev.json'\n",
    "    test_data_path = './tnews/test.json'\n",
    "    label_data_path = './tnews/labels.json'\n",
    "    bert_name = \"hfl/chinese-roberta-wwm-ext\"\n",
    "    max_length  = 50\n",
    "    batch_size = 8\n",
    "    epochs = 2\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(config,data_type='train',nums_sample = -1):\n",
    "    text = []\n",
    "    label = []\n",
    "    data_path = {\"train\":config.train_data_path,\n",
    "                \"dev\":config.dev_data_path,\n",
    "                \"test\":config.test_data_path}[data_type]\n",
    "    with open(data_path, encoding='utf-8')as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.strip()\n",
    "            dic = json.loads(line)\n",
    "            text.append(dic['sentence'])\n",
    "            label.append(dic['label'])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = text\n",
    "    df['label'] = label\n",
    "    df  = shuffle(df).head(nums_sample)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_dict(config):\n",
    "    label_dict = {}\n",
    "    with open(config.label_data_path, encoding='utf-8')as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.strip()\n",
    "            dic = json.loads(line)\n",
    "            label_dict[dic['label']]=dic['label_desc']\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_raw_data(config,\"train\",100)\n",
    "df_dev = get_raw_data(config,\"dev\",100)\n",
    "labels_dict = get_labels_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30569</th>\n",
       "      <td>资本家的资本是怎样来的？</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14546</th>\n",
       "      <td>山西的大院，哪个最值得去？</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52649</th>\n",
       "      <td>号外｜最新太阳能发电、储能百亿富豪榜：22家公司30人上榜</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10791</th>\n",
       "      <td>汽车大灯发黄模糊不够亮？别傻傻被4S店忽悠，教你3招轻松搞定！</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50220</th>\n",
       "      <td>「封口避谈强吻事件」麦明诗透视tube dress力压师姐</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text label\n",
       "30569                     资本家的资本是怎样来的？   104\n",
       "14546                    山西的大院，哪个最值得去？   112\n",
       "52649    号外｜最新太阳能发电、储能百亿富豪榜：22家公司30人上榜   109\n",
       "10791  汽车大灯发黄模糊不够亮？别傻傻被4S店忽悠，教你3招轻松搞定！   107\n",
       "50220    「封口避谈强吻事件」麦明诗透视tube dress力压师姐   102"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100': 'news_story',\n",
       " '101': 'news_culture',\n",
       " '102': 'news_entertainment',\n",
       " '103': 'news_sports',\n",
       " '104': 'news_finance',\n",
       " '106': 'news_house',\n",
       " '107': 'news_car',\n",
       " '108': 'news_edu',\n",
       " '109': 'news_tech',\n",
       " '110': 'news_military',\n",
       " '112': 'news_travel',\n",
       " '113': 'news_world',\n",
       " '114': 'news_stock',\n",
       " '115': 'news_agriculture',\n",
       " '116': 'news_game'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Process Data According to Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {'100': '民生', '101': '文化', '102': '娱乐', '103': '体育', '104': '财经', '106': '房产', '107': '汽车',\n",
    "                          '108': '教育', '109': '科技', '110': '军事', '112': '旅游', '113': '国际', '114': '证券', '115': '农业',\n",
    "                          '116': '游戏'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = {k: i for i, (k, v) in enumerate(label_dic.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx_dic = {k: tokenizer.convert_tokens_to_ids(list(v)) for k, v in label_dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_idx_dic #these tokens to fill prompt tempalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_length(text):\n",
    "    encoding = tokenizer(text)\n",
    "    return len(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30569</th>\n",
       "      <td>资本家的资本是怎样来的？</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14546</th>\n",
       "      <td>山西的大院，哪个最值得去？</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52649</th>\n",
       "      <td>号外｜最新太阳能发电、储能百亿富豪榜：22家公司30人上榜</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10791</th>\n",
       "      <td>汽车大灯发黄模糊不够亮？别傻傻被4S店忽悠，教你3招轻松搞定！</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50220</th>\n",
       "      <td>「封口避谈强吻事件」麦明诗透视tube dress力压师姐</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text label\n",
       "30569                     资本家的资本是怎样来的？   104\n",
       "14546                    山西的大院，哪个最值得去？   112\n",
       "52649    号外｜最新太阳能发电、储能百亿富豪榜：22家公司30人上榜   109\n",
       "10791  汽车大灯发黄模糊不够亮？别傻傻被4S店忽悠，教你3招轻松搞定！   107\n",
       "50220    「封口避谈强吻事件」麦明诗透视tube dress力压师姐   102"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len_list = df_train['text'].apply(lambda x:get_token_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  5., 13., 14., 17., 10., 13., 17.,  3.,  4.]),\n",
       " array([ 9. , 11.9, 14.8, 17.7, 20.6, 23.5, 26.4, 29.3, 32.2, 35.1, 38. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(token_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ids(text):\n",
    "    encoding = tokenizer(text = text,\n",
    "                         max_length = config.max_length,\n",
    "                         padding=\"max_length\",\n",
    "                         truncation=True,\n",
    "                         return_tensors = \"pt\",)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pos = [5, 6] # more one because cls token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '播报一则[MASK][MASK]新闻：'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 3064, 2845, 671, 1156, 103, 103, 3173, 7319, 8038, 102]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer(\"播报一则[MASK][MASK]新闻：\")['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '播', '报', '一', '则', '[MASK]', '[MASK]', '新', '闻', '：', '[SEP]']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer(\"播报一则[MASK][MASK]新闻：\")['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/WySJb0s.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs_process(data_text, data_label,mask_pos = mask_pos):\n",
    "\n",
    "    #     mask_pos = [5, 6]\n",
    "\n",
    "    encodings = []\n",
    "    labels = []\n",
    "    for text, label in zip(data_text, data_label):\n",
    "        text = prompt_template + text # templata\n",
    "        encoding = get_token_ids(text)\n",
    "        encodings.append(encoding) \n",
    "        labels.append(label_index[label])\n",
    "\n",
    "    item = {}\n",
    "    for encoding in encodings:\n",
    "        for key in ['input_ids', 'attention_mask']:\n",
    "            if key in item.keys():\n",
    "                item[key].append(encoding[key])\n",
    "            else:\n",
    "                item[key] = [encoding[key]]\n",
    "\n",
    "    for key in ['input_ids', 'attention_mask']:\n",
    "        item[key] = torch.cat(item[key])\n",
    "        \n",
    "    return item, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(df):\n",
    "    text  = df['text']\n",
    "    label  = df['label']\n",
    "    \n",
    "    encoding, label = inputs_process(text, label)\n",
    "    \n",
    "    return DataLoader(Dataset(encoding, label), config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_data_loader(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = get_data_loader(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Prompt Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.models.bert.modeling_bert.BertModel\n",
    "from transformers import RobertaConfig, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertPreTrainedModel\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
    "\n",
    "\n",
    "class BertForPromptTuning(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config: RobertaConfig):\n",
    "        \n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = RobertaModel(config, add_pooling_layer=False)  # backbone\n",
    "        self.cls = BertOnlyMLMHead(config)  # MLM vocab multi-classification\n",
    "        self.init_weights()\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        \"\"\"Use word embedding weight as the output weight in the cls layer\"\"\"\n",
    "        self.cls.predictions.decoder.weight = new_embeddings.weight\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        self.set_output_embeddings(self.bert.embeddings.word_embeddings)\n",
    "\n",
    "        # take the input ids embedding values\n",
    "        inputs_embeds = self.bert.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # outputs[0] -> shape [batch_size,max_length,hidden_size (768)]\n",
    "        logits = self.cls(outputs[0])\n",
    "        # logits:  -> shape [batch_size,max_length,vocab_size]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            s = attention_mask.shape[0] * attention_mask.shape[1]\n",
    "            loss = loss_fct(logits.view(s, -1), labels.view(-1))\n",
    "\n",
    "        # logits: tokens out-> [batch_size,max_length,vocab_size]\n",
    "        # pool out: outputs[1:] -> [batch_size,max_length,hidden_size]\n",
    "        # cls : outputs[0][:, 0] -> first token cls output [batch_size,hidden_size]\n",
    "        output = (logits,) + outputs[1:] + (outputs[0][:, 0],)\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name = config.bert_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForPromptTuning: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForPromptTuning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForPromptTuning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForPromptTuning were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForPromptTuning.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = sample['input_ids'].to(config.device)\n",
    "attention_mask = sample['attention_mask'].to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_logits = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = label_idx_dic.values()\n",
    "label_idx_0, label_idx_1 = zip(*label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_idx_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_logits = token_logits[:, mask_pos[0]][:, label_idx_0] + \\\n",
    "              token_logits[:, mask_pos[1]][:, label_idx_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 15])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.7480,  6.9740, 13.6546, 10.3883, 18.3691,  6.9044,  9.0074,  8.8410,\n",
       "        12.2246,  9.5361,  6.5133, 14.2723,  7.1044,  9.2364,  6.4415],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  2,  4,  6,  2, 11,  2, 11], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(mask_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_logits = mask_logits[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.7480,  6.9740, 13.6546, 10.3883, 18.3691,  6.9044,  9.0074,  8.8410,\n",
       "         12.2246,  9.5361,  6.5133, 14.2723,  7.1044,  9.2364,  6.4415]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =sample['labels'].to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fct(mask_logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([0]).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fct(mask_logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6497, device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(data):\n",
    "    \n",
    "    input_ids = data['input_ids'].to(config.device)\n",
    "    attention_mask = data['attention_mask'].to(config.device)\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    label_idx = label_idx_dic.values()\n",
    "    label_idx_0, label_idx_1 = zip(*label_idx)\n",
    "\n",
    "    token_logits = out[0]\n",
    "    # Take the vocab predictions, add together and take argmax\n",
    "    mask_logits = token_logits[:, mask_pos[0]][:, label_idx_0] + \\\n",
    "                  token_logits[:, mask_pos[1]][:, label_idx_1]\n",
    "    \n",
    "    y_pred = torch.argmax(mask_logits, dim=1)\n",
    "    y_true = data['labels'].to(config.device)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(mask_logits, y_true)\n",
    "\n",
    "    return loss, y_pred.cpu().numpy(), y_true.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(all_label,all_pred):\n",
    "    \n",
    "    acc = accuracy_score(all_label, all_pred)\n",
    "    \n",
    "    return acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(mode,loss,acc):\n",
    "    result_str = f'{mode} loss:{loss:.4f} acc:{acc:.4f}'\n",
    "    print(result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(loader):\n",
    "    train_loss = 0\n",
    "    all_label = []\n",
    "    all_pred = []\n",
    "    pbar = tqdm(loader)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        loss, output, label = train_step(batch)\n",
    "        if output is not None and label is not None:\n",
    "            all_label.extend(label)\n",
    "            all_pred.extend(output)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pbar.update()\n",
    "        pbar.set_description(f'loss:{loss.item():.4f}')\n",
    "\n",
    "    print_metrics('train', train_loss / len(loader),calculate_metrics(all_label, all_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_func(loader):\n",
    "\n",
    "    dev_loss = 0\n",
    "\n",
    "    all_label = []\n",
    "    all_pred = []\n",
    "    metrics = {}\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            loss, output, label = train_step(batch)\n",
    "            dev_loss += loss.item()\n",
    "            if output is not None:\n",
    "                all_label.extend(label)\n",
    "                all_pred.extend(output)\n",
    "\n",
    "    if all_pred is not None:\n",
    "        acc = calculate_metrics(all_label, all_pred)\n",
    "        print_metrics(\"dev\", dev_loss / len(loader), acc)\n",
    "        target_names = [f'class {i}' for i in range(len(set(all_label)))]\n",
    "#         print(classification_report(all_label, all_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********epoch: 1***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.5513: 100%|█████████████████████████████████████████████████████████████████████| 13/13 [00:11<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.4800 acc:0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev loss:2.0098 acc:0.5100\n",
      "***********epoch: 2***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.0888: 100%|█████████████████████████████████████████████████████████████████████| 13/13 [00:11<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.5118 acc:0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev loss:1.9708 acc:0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config.epochs):\n",
    "    \n",
    "    print(f'***********epoch: {epoch + 1}***********')\n",
    "    \n",
    "    train_func(train_loader)\n",
    "    \n",
    "    dev_func(dev_loader)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
